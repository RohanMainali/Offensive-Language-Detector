{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11696386,"sourceType":"datasetVersion","datasetId":7341277}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Offensive Content Detection with Contextual Understanding\n# --------------------------------------------------------\n# Enhanced with advanced suggestion generation using pre-trained models\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport pickle\nimport time\nimport requests\nfrom io import StringIO\nfrom tqdm.notebook import tqdm\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n\n# Download necessary NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\nprint(\"Setting up the environment...\")\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:50:48.439723Z","iopub.execute_input":"2025-05-06T12:50:48.439976Z","iopub.status.idle":"2025-05-06T12:51:16.533905Z","shell.execute_reply.started":"2025-05-06T12:50:48.439957Z","shell.execute_reply":"2025-05-06T12:51:16.533109Z"}},"outputs":[{"name":"stderr","text":"2025-05-06 12:51:03.972014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746535864.234549      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746535864.312825      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Setting up the environment...\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nprint(\"Loading dataset...\")\nfile_path = \"/kaggle/input/offensivetext/train.csv\"\ndata = pd.read_csv(file_path)\n\nprint(f\"Dataset loaded with shape: {data.shape}\")\nprint(\"\\nFirst few rows of the dataset:\")\nprint(data.head())\n\n# Display basic statistics\nprint(\"\\nBasic statistics of the dataset:\")\nprint(data.describe())\n\n# Check for missing values\nprint(\"\\nMissing values in each column:\")\nprint(data.isnull().sum())\n\n# Distribution of labels\nprint(\"\\nDistribution of labels:\")\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f\"{col}: {data[col].value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:51:19.654447Z","iopub.execute_input":"2025-05-06T12:51:19.655022Z","iopub.status.idle":"2025-05-06T12:51:21.304706Z","shell.execute_reply.started":"2025-05-06T12:51:19.654997Z","shell.execute_reply":"2025-05-06T12:51:21.304021Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded with shape: (159571, 8)\n\nFirst few rows of the dataset:\n                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  \n\nBasic statistics of the dataset:\n               toxic   severe_toxic        obscene         threat  \\\ncount  159571.000000  159571.000000  159571.000000  159571.000000   \nmean        0.095844       0.009996       0.052948       0.002996   \nstd         0.294379       0.099477       0.223931       0.054650   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n              insult  identity_hate  \ncount  159571.000000  159571.000000  \nmean        0.049364       0.008805  \nstd         0.216627       0.093420  \nmin         0.000000       0.000000  \n25%         0.000000       0.000000  \n50%         0.000000       0.000000  \n75%         0.000000       0.000000  \nmax         1.000000       1.000000  \n\nMissing values in each column:\nid               0\ncomment_text     0\ntoxic            0\nsevere_toxic     0\nobscene          0\nthreat           0\ninsult           0\nidentity_hate    0\ndtype: int64\n\nDistribution of labels:\ntoxic: toxic\n0    144277\n1     15294\nName: count, dtype: int64\nsevere_toxic: severe_toxic\n0    157976\n1      1595\nName: count, dtype: int64\nobscene: obscene\n0    151122\n1      8449\nName: count, dtype: int64\nthreat: threat\n0    159093\n1       478\nName: count, dtype: int64\ninsult: insult\n0    151694\n1      7877\nName: count, dtype: int64\nidentity_hate: identity_hate\n0    158166\n1      1405\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Create a binary toxicity label (any type of toxicity)\ndata['any_toxic'] = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\nprint(\"\\nOverall toxicity distribution:\")\nprint(data['any_toxic'].value_counts())\nprint(f\"Percentage of toxic comments: {data['any_toxic'].mean() * 100:.2f}%\")\n\n# Preprocess text data\ndef preprocess_text(text):\n    \"\"\"Basic preprocessing for text data\"\"\"\n    if isinstance(text, str):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove URLs\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n        # Remove HTML tags\n        text = re.sub(r'<.*?>', '', text)\n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    return \"\"\n\n# Sample some comments to examine\nprint(\"\\nSample non-toxic comments:\")\nnon_toxic_samples = data[data['any_toxic'] == 0]['comment_text'].sample(5).tolist()\nfor i, comment in enumerate(non_toxic_samples):\n    print(f\"{i+1}. {comment[:100]}...\")\n\nprint(\"\\nSample toxic comments:\")\ntoxic_samples = data[data['any_toxic'] == 1]['comment_text'].sample(5).tolist()\nfor i, comment in enumerate(toxic_samples):\n    print(f\"{i+1}. {comment[:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:51:25.190658Z","iopub.execute_input":"2025-05-06T12:51:25.190929Z","iopub.status.idle":"2025-05-06T12:51:25.247174Z","shell.execute_reply.started":"2025-05-06T12:51:25.190909Z","shell.execute_reply":"2025-05-06T12:51:25.246368Z"}},"outputs":[{"name":"stdout","text":"\nOverall toxicity distribution:\nany_toxic\n0    143346\n1     16225\nName: count, dtype: int64\nPercentage of toxic comments: 10.17%\n\nSample non-toxic comments:\n1. \"\n\nOh, don't worry about me, Sandstein. I'm of no strong opinion as to what is \"\"well.\"\" Editing Wik...\n2. Are you trying to dispute that fact?...\n3. SWOT analysis \n\nThis source – Align Technology, Inc. SWOT Analysis. (2013). 1-8. – is used 11 times,...\n4. cover \n\nso, do we want a current or older cover? i have the 57 paper edition i can scan. its actuall...\n5. P.S. It's probably worth setting up a template to be used for all the periods, so they can easily be...\n\nSample toxic comments:\n1. Sleazebag \n\nJimbo, you are a sleazebag. Please go away....\n2. The above user, who has failed to sign, is ignorant. The term 'nascent hydrogen' has a respectable s...\n3. Don't worry, I'm in the mood for an edit war!  Let's fight this dirty communist!  Long live Romania!...\n4. Dude \n\nWhy do you fucking delete my shit when I didn't fucking do anything to you retard!...\n5. \"\n  The Anti-Vandalism BarnstarFCUK YOU Doc9871 FOOL 122.111.252.2  \n\"...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Prepare the data for model training\nprint(\"\\nPreparing data for model training...\")\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data['comment_text'], \n    data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']], \n    test_size=0.2, \n    random_state=42\n)\n\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Testing set size: {len(X_test)}\")\n\n# Fine-tune a pre-trained model (RoBERTa)\nMODEL_NAME = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Create a custom dataset\nclass ToxicityDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n        }\n        \n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels.iloc[idx].values, dtype=torch.float)\n            \n        return item\n\n# Create the datasets\ntrain_dataset = ToxicityDataset(X_train, y_train, tokenizer)\ntest_dataset = ToxicityDataset(X_test, y_test, tokenizer)\n\n# Create dataloaders\nBATCH_SIZE = 16\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:51:26.466989Z","iopub.execute_input":"2025-05-06T12:51:26.467690Z","iopub.status.idle":"2025-05-06T12:51:27.500939Z","shell.execute_reply.started":"2025-05-06T12:51:26.467657Z","shell.execute_reply":"2025-05-06T12:51:27.500128Z"}},"outputs":[{"name":"stdout","text":"\nPreparing data for model training...\nTraining set size: 127656\nTesting set size: 31915\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854d07cd58c64990929d5f4a7d63695c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83699a2c9a414dbfb6a0886a4175282d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7485d88283014345b5186bbc8a20c140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b91684678846de81689a5c27224097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887e4a4c4eb74dedb9994ebeeae1f949"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Initialize the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n)\nmodel.to(device)\n\n# Training parameters\nEPOCHS = 1\noptimizer = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# Training function\ndef train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    \n    progress_bar = tqdm(dataloader, desc=\"Training\")\n    for batch in progress_bar:\n        # Move batch to device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        # Update progress bar\n        progress_bar.set_postfix({'loss': loss.item()})\n    \n    return total_loss / len(dataloader)\n\n# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids\n            )\n            \n            logits = outputs.logits\n            predictions.extend(torch.sigmoid(logits).cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    \n    predictions = np.array(predictions) >= 0.5\n    true_labels = np.array(true_labels) >= 0.5\n    \n    return predictions, true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:51:30.519867Z","iopub.execute_input":"2025-05-06T12:51:30.520359Z","iopub.status.idle":"2025-05-06T12:51:33.554206Z","shell.execute_reply.started":"2025-05-06T12:51:30.520338Z","shell.execute_reply":"2025-05-06T12:51:33.553483Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901abcb4201948388c6038086138d48b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Train the model\nprint(\"\\nTraining the model...\")\ntrain_losses = []\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    start_time = time.time()\n    \n    # Train\n    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n    train_losses.append(train_loss)\n    \n    # Evaluate\n    print(\"Evaluating...\")\n    predictions, true_labels = evaluate(model, test_dataloader, device)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(true_labels.flatten(), predictions.flatten())\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        true_labels.flatten(), \n        predictions.flatten(), \n        average='binary'\n    )\n    \n    # Print metrics\n    print(f\"Epoch {epoch+1} completed in {time.time() - start_time:.2f} seconds\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n    \n    # Print per-category metrics\n    for i, category in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n        cat_precision, cat_recall, cat_f1, _ = precision_recall_fscore_support(\n            true_labels[:, i], \n            predictions[:, i], \n            average='binary'\n        )\n        print(f\"{category}: Precision={cat_precision:.4f}, Recall={cat_recall:.4f}, F1={cat_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:51:35.558103Z","iopub.execute_input":"2025-05-06T12:51:35.558378Z","iopub.status.idle":"2025-05-06T13:50:45.363694Z","shell.execute_reply.started":"2025-05-06T12:51:35.558359Z","shell.execute_reply":"2025-05-06T13:50:45.362865Z"}},"outputs":[{"name":"stdout","text":"\nTraining the model...\nEpoch 1/1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/7979 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9186f6b63654dea98e068e541831c90"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1995 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0caf47ac754586b964671265df2140"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 completed in 3549.74 seconds\nTrain Loss: 0.0480\nAccuracy: 0.9847, Precision: 0.7971, Recall: 0.7870, F1: 0.7920\ntoxic: Precision=0.8463, Recall=0.8289, F1=0.8375\nsevere_toxic: Precision=0.5227, Recall=0.4299, F1=0.4718\nobscene: Precision=0.8271, Recall=0.8536, F1=0.8402\nthreat: Precision=0.5077, Recall=0.4459, F1=0.4748\ninsult: Precision=0.7513, Recall=0.7900, F1=0.7702\nidentity_hate: Precision=0.6359, Recall=0.4218, F1=0.5072\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Save the model\nprint(\"\\nSaving the model...\")\nmodel_path = \"offensive_content_detection_model\"\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:59:25.051027Z","iopub.execute_input":"2025-05-06T13:59:25.051723Z","iopub.status.idle":"2025-05-06T13:59:26.308784Z","shell.execute_reply.started":"2025-05-06T13:59:25.051690Z","shell.execute_reply":"2025-05-06T13:59:26.308176Z"}},"outputs":[{"name":"stdout","text":"\nSaving the model...\nModel saved to offensive_content_detection_model\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!zip -r output.zip /kaggle/working/\nfrom IPython.display import FileLink\n\n# Create a clickable download link\nFileLink(r'output.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:59:43.487843Z","iopub.execute_input":"2025-05-06T13:59:43.488322Z","iopub.status.idle":"2025-05-06T14:00:09.740566Z","shell.execute_reply.started":"2025-05-06T13:59:43.488299Z","shell.execute_reply":"2025-05-06T14:00:09.739831Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/offensive_content_detection_model/ (stored 0%)\n  adding: kaggle/working/offensive_content_detection_model/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/offensive_content_detection_model/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 8%)\n  adding: kaggle/working/offensive_content_detection_model/merges.txt (deflated 53%)\n  adding: kaggle/working/offensive_content_detection_model/vocab.json (deflated 59%)\n  adding: kaggle/working/offensive_content_detection_model/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/offensive_content_detection_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/offensive_content_detection_model/config.json (deflated 55%)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output.zip","text/html":"<a href='output.zip' target='_blank'>output.zip</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Load T5 model for text rewriting\nprint(\"\\nLoading T5 model for generating non-offensive alternatives...\")\nt5_model_name = \"t5-base\"\nt5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\nt5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\nt5_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:00:52.444734Z","iopub.execute_input":"2025-05-06T14:00:52.445009Z","iopub.status.idle":"2025-05-06T14:00:59.462299Z","shell.execute_reply.started":"2025-05-06T14:00:52.444990Z","shell.execute_reply":"2025-05-06T14:00:59.461572Z"}},"outputs":[{"name":"stdout","text":"\nLoading T5 model for generating non-offensive alternatives...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cfd25a341a94981b3029278238ab4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24871e1b898a48e1bddcb4021f0300c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8775019f711c4445b06e049fcb646ece"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a0611fd9c74c5d91bb90b2f779901e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15722d4e6fc7472bbd5bb8512c6b32b9"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def analyze_context(text, toxicity_scores):\n    \"\"\"\n    Completely redesigned context analysis with direct insult detection\n    \"\"\"\n    # EDIT: First check for direct insults and attacks - these override other contexts\n    direct_insult_patterns = [\n        r'you(\\'re| are) (a |an |such a )?(fucking |damn |stupid |idiotic |dumb |)?(idiot|stupid|dumb|moron|asshole|bitch|cunt|bastard|retard)',\n        r'(fuck|screw|damn) you',\n        r'i hate you',\n        r'(go|get) (fuck|screw) yourself',\n        r'(go|get) lost',\n        r'(go|get) (the fuck)? out( of here)?',\n        r'kill yourself',\n        r'die',\n        r'nobody (likes|cares about) you',\n        r'you(\\'re| are) (worthless|useless|pathetic)',\n        r'you (suck|stink)',\n        r'you(\\'re| are) (a |an )?(fucking |damn |stupid |)?(joke|loser|failure)',\n        r'shut (the fuck )?up',\n        r'shut your (mouth|face)',\n        r'(fuck|screw|damn) (off|you)',\n        r'(you|your) (stupid|dumb|idiotic|moronic) (fuck|ass|bitch)',\n        r'what (the fuck|the hell) (is|are) (you|this)'\n    ]\n    \n    # Check for direct insults first - these override other contexts\n    has_direct_insult = any(re.search(pattern, text.lower()) for pattern in direct_insult_patterns)\n    \n    # If it's a direct insult, return immediately with no context adjustment\n    if has_direct_insult:\n        return {\n            'original_scores': toxicity_scores,\n            'adjusted_scores': toxicity_scores,  # No adjustment for direct insults\n            'context_adjustment': 0,\n            'reasons': [\"Direct insult or attack detected\"],\n            'context_type': {\n                'direct_insult': True,\n                'educational': False,\n                'condemning': False,\n                'quoting': False\n            }\n        }\n    \n    # If not a direct insult, proceed with normal context detection\n    educational_keywords = [\n        'research', 'study', 'learn', 'educate', 'inform', 'article', 'documentary',\n        'paper', 'academic', 'lecture', 'class', 'teach', 'professor', 'student',\n        'history', 'historical', 'analyze', 'analysis', 'examine', 'investigate',\n        'discuss', 'discussed', 'explains', 'explained', 'report', 'reported'\n    ]\n    \n    condemning_keywords = [\n        'bad', 'wrong', 'terrible', 'shouldn\\'t', 'should not', 'condemn', 'against', 'oppose',\n        'harmful', 'unacceptable', 'inappropriate', 'immoral', 'evil', 'horrific', 'awful',\n        'disgusting', 'reject', 'denounce', 'criticize', 'disagree', 'disapprove',\n        'never', 'not', 'isn\\'t', 'aren\\'t', 'don\\'t', 'doesn\\'t', 'shouldn\\'t', 'wouldn\\'t',\n        'must not', 'cannot', 'can\\'t', 'stop', 'prevent', 'fight against', 'combat'\n    ]\n    \n    # EDIT: More precise quoting indicators that require clear attribution\n    quoting_indicators = [\n        'said', 'quoted', 'according to', 'reported', 'states', 'stated',\n        'mentioned', 'wrote', 'writes', 'claim', 'claims', 'argue', 'argues',\n        'testimony', 'quote', 'citing', 'cited', 'reference', 'referenced'\n    ]\n    \n    # EDIT: Require explicit quotation marks or attribution phrases for quoting context\n    explicit_quoting_patterns = [\n        r'\"[^\"]+\"',                      # Double quotes\n        r\"'[^']+'\",                      # Single quotes\n        r\"said[^.]*['\\\"]\",               # Said followed by quote\n        r\"quoted[^.]*['\\\"]\",             # Quoted followed by quote\n        r\"according to[^,.]+(,|\\.)\",     # According to someone\n        r\"reported that[^,.]+(,|\\.)\",    # Reported that\n        r\"testified that[^,.]+(,|\\.)\",   # Testified that\n        r\"statement[^,.]+(,|\\.)\",        # Statement from\n        r\"example of[^,.]+(,|\\.)\"        # Example of\n    ]\n    \n    educational_patterns = [\n        r'in (a|the) (documentary|article|paper|study|research|book|lecture|class)',\n        r'(documentary|article|paper|study|research) (on|about|discussing)',\n        r'(learn|learned|learning) about',\n        r'(teach|taught|teaching) (about|how)',\n        r'(discuss|discussed|discussing) (how|why|the)',\n        r'(explain|explained|explaining) (how|why|the)',\n        r'(study|studied|studying) (of|on|about)',\n        r'(research|researched|researching) (on|about)'\n    ]\n    \n    condemning_patterns = [\n        r'(is|are|was|were) (bad|wrong|terrible|harmful|unacceptable)',\n        r'should (not|never) be',\n        r'(must|should|need to) (stop|prevent|avoid|condemn)',\n        r'(don\\'t|do not|doesn\\'t|does not) (condone|accept|approve)',\n        r'(fight|stand) against',\n        r'(oppose|opposing|opposed to)',\n        r'(reject|rejecting|rejected)',\n        r'(criticize|criticizing|criticized)'\n    ]\n    \n    # Check for educational context\n    has_educational_context = any(keyword in text.lower() for keyword in educational_keywords)\n    has_educational_pattern = any(re.search(pattern, text.lower()) for pattern in educational_patterns)\n    \n    # Check for condemning context\n    has_condemning_context = any(keyword in text.lower() for keyword in condemning_keywords)\n    has_condemning_pattern = any(re.search(pattern, text.lower()) for pattern in condemning_patterns)\n    \n    # EDIT: More stringent check for quoting context - require explicit quotation\n    has_quoting_keyword = any(indicator in text.lower() for indicator in quoting_indicators)\n    has_explicit_quote = any(re.search(pattern, text) for pattern in explicit_quoting_patterns)\n    has_quoting_context = has_quoting_keyword and has_explicit_quote\n    \n    # Context scoring\n    context_score = 0\n    reasons = []\n    \n    # Educational context scoring\n    if has_educational_context or has_educational_pattern:\n        context_score -= 0.4\n        reasons.append(\"Educational context detected\")\n    \n    # Condemning context scoring\n    if has_condemning_context or has_condemning_pattern:\n        context_score -= 0.5\n        reasons.append(\"Condemning harmful behavior\")\n    \n    # Quoting context scoring - only if explicit quotation is present\n    if has_quoting_context:\n        context_score -= 0.4\n        reasons.append(\"Quoting or reporting context\")\n    \n    # Special case handling for common phrases that are incorrectly flagged\n    special_phrases = {\n        \"rape is bad\": -0.9,\n        \"rape is wrong\": -0.9,\n        \"rape is terrible\": -0.9,\n        \"sexual assault is\": -0.9,\n        \"hate speech is\": -0.9,\n        \"racism is\": -0.9,\n        \"sexism is\": -0.9,\n        \"homophobia is\": -0.9,\n        \"transphobia is\": -0.9,\n        \"discrimination is\": -0.9,\n        \"harassment is\": -0.9,\n        \"bullying is\": -0.9\n    }\n    \n    for phrase, adjustment in special_phrases.items():\n        if phrase in text.lower():\n            context_score += adjustment\n            reasons.append(f\"Special case: condemning phrase '{phrase}'\")\n    \n    # Apply stronger adjustments for combined contexts\n    if len(reasons) > 1:\n        context_score -= 0.2\n        reasons.append(\"Multiple contextual indicators strengthen non-offensive determination\")\n    \n    # Ensure the adjustment doesn't go below a reasonable threshold\n    context_score = max(context_score, -0.9)\n    \n    # Apply the context adjustment to each toxicity score\n    adjusted_scores = {\n        label: max(0, min(1, score + context_score)) \n        for label, score in toxicity_scores.items()\n    }\n    \n    return {\n        'original_scores': toxicity_scores,\n        'adjusted_scores': adjusted_scores,\n        'context_adjustment': context_score,\n        'reasons': reasons,\n        'context_type': {\n            'direct_insult': False,\n            'educational': has_educational_context or has_educational_pattern,\n            'condemning': has_condemning_context or has_condemning_pattern,\n            'quoting': has_quoting_context\n        }\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:24:34.223391Z","iopub.execute_input":"2025-05-06T14:24:34.224003Z","iopub.status.idle":"2025-05-06T14:24:34.238312Z","shell.execute_reply.started":"2025-05-06T14:24:34.223979Z","shell.execute_reply":"2025-05-06T14:24:34.237549Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Function to identify offensive words in text\ndef identify_offensive_words(text, context_type=None):\n    \"\"\"\n    Enhanced offensive word detection with better context handling\n    \"\"\"\n    # EDIT: Check for direct insults first - these are always offensive regardless of context\n    if context_type and context_type.get('direct_insult', False):\n        # Extract the offensive words from the direct insult\n        words = re.findall(r'\\b\\w+\\b', text.lower())\n        offensive_words = {}\n        \n        # Common offensive words in direct insults\n        insult_words = {\n            'fuck': 0.9, 'fucking': 0.9, 'fucked': 0.9, 'fucker': 0.9,\n            'shit': 0.8, 'shitty': 0.8, 'bullshit': 0.7,\n            'ass': 0.7, 'asshole': 0.9, 'jackass': 0.8,\n            'bitch': 0.9, 'bitches': 0.9, 'bitching': 0.8,\n            'damn': 0.6, 'goddamn': 0.7,\n            'nigga': 1.0, 'nigger': 1.0, 'negro': 0.9,\n            'whore': 0.9, 'slut': 0.9, 'hoe': 0.9,\n            'cunt': 1.0, 'pussy': 0.8, 'dick': 0.8, 'cock': 0.8,\n            'retard': 0.9, 'retarded': 0.9,\n            'faggot': 1.0, 'fag': 0.9, 'homo': 0.8,\n            'idiot': 0.7, 'stupid': 0.7, 'dumb': 0.7, 'moron': 0.8,\n            'kill': 0.9, 'die': 0.9, 'suicide': 0.9,\n            'hate': 0.8, 'loser': 0.7\n        }\n        \n        for word in words:\n            clean_word = ''.join(c for c in word if c.isalnum())\n            if clean_word in insult_words:\n                offensive_words[clean_word] = insult_words[clean_word]\n        \n        return offensive_words\n    \n    # Common offensive words dictionary with severity scores (0-1)\n    offensive_words = {\n        'fuck': 0.8, 'fucking': 0.8, 'fucked': 0.8, 'fucker': 0.9,\n        'shit': 0.6, 'shitty': 0.7, 'bullshit': 0.6,\n        'ass': 0.5, 'asshole': 0.8, 'jackass': 0.7,\n        'bitch': 0.8, 'bitches': 0.8, 'bitching': 0.7,\n        'damn': 0.4, 'goddamn': 0.6,\n        'nigga': 0.9, 'nigger': 1.0, 'negro': 0.8,\n        'whore': 0.9, 'slut': 0.9, 'hoe': 0.8,\n        'cunt': 0.9, 'pussy': 0.8, 'dick': 0.7, 'cock': 0.7,\n        'retard': 0.9, 'retarded': 0.9,\n        'faggot': 1.0, 'fag': 0.9, 'homo': 0.7,\n        'idiot': 0.5, 'stupid': 0.5, 'dumb': 0.5, 'moron': 0.6,\n        'kill': 0.7, 'die': 0.6, 'suicide': 0.8,\n        'hate': 0.6, 'loser': 0.5\n    }\n    \n    # Words that should be treated differently in educational/condemning contexts\n    contextual_words = {\n        'rape': 0.9,\n        'sexual assault': 0.9,\n        'racist': 0.8,\n        'sexist': 0.8,\n        'homophobic': 0.8,\n        'transphobic': 0.8,\n        'nazi': 0.9,\n        'terrorism': 0.8,\n        'terrorist': 0.8,\n        'genocide': 0.9,\n        'slavery': 0.8,\n        'holocaust': 0.9\n    }\n    \n    found_words = {}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Check for multi-word offensive terms\n    for term in contextual_words:\n        if ' ' in term and term in text.lower():\n            # Only add if not in an educational or condemning context\n            if not context_type or (not context_type.get('educational', False) and \n                                   not context_type.get('condemning', False) and\n                                   not context_type.get('quoting', False)):\n                found_words[term] = contextual_words[term]\n    \n    # Check for single offensive words\n    for word in words:\n        # Clean word of punctuation for matching\n        clean_word = ''.join(c for c in word if c.isalnum())\n        \n        if clean_word in offensive_words:\n            # EDIT: For quoting context, only include if the word is extremely offensive\n            if context_type and context_type.get('quoting', False):\n                if offensive_words[clean_word] >= 0.9:  # Only include highly offensive words in quotes\n                    found_words[clean_word] = offensive_words[clean_word]\n            else:\n                found_words[clean_word] = offensive_words[clean_word]\n        elif clean_word in contextual_words:\n            # Only add contextual words if not in an educational or condemning context\n            if not context_type or (not context_type.get('educational', False) and \n                                   not context_type.get('condemning', False) and\n                                   not context_type.get('quoting', False)):\n                found_words[clean_word] = contextual_words[clean_word]\n    \n    return found_words\n\n# Advanced function to generate non-offensive alternatives using T5\ndef predict_toxicity(text, model, tokenizer, device):\n    \"\"\"\n    Completely redesigned toxicity prediction with better direct insult detection\n    \"\"\"\n    # EDIT: First check for direct insults using pattern matching\n    direct_insult_patterns = [\n        r'you(\\'re| are) (a |an |such a )?(fucking |damn |stupid |idiotic |dumb |)?(idiot|stupid|dumb|moron|asshole|bitch|cunt|bastard|retard)',\n        r'(fuck|screw|damn) you',\n        r'i hate you',\n        r'(go|get) (fuck|screw) yourself',\n        r'(go|get) lost',\n        r'(go|get) (the fuck)? out( of here)?',\n        r'kill yourself',\n        r'die',\n        r'nobody (likes|cares about) you',\n        r'you(\\'re| are) (worthless|useless|pathetic)',\n        r'you (suck|stink)',\n        r'you(\\'re| are) (a |an )?(fucking |damn |stupid |)?(joke|loser|failure)',\n        r'shut (the fuck )?up',\n        r'shut your (mouth|face)',\n        r'(fuck|screw|damn) (off|you)',\n        r'(you|your) (stupid|dumb|idiotic|moronic) (fuck|ass|bitch)',\n        r'what (the fuck|the hell) (is|are) (you|this)'\n    ]\n    \n    # Check if text matches any direct insult pattern\n    is_direct_insult = any(re.search(pattern, text.lower()) for pattern in direct_insult_patterns)\n    \n    # If it's a direct insult, we can skip the model prediction and mark as toxic\n    if is_direct_insult:\n        # Create dummy toxicity scores (high for all categories)\n        toxicity_scores = {\n            'toxic': 0.9,\n            'severe_toxic': 0.7,\n            'obscene': 0.8,\n            'threat': 0.5,\n            'insult': 0.9,\n            'identity_hate': 0.5\n        }\n        \n        # Get contextual analysis (which will detect the direct insult)\n        contextual_analysis = analyze_context(text, toxicity_scores)\n        \n        # Identify offensive words\n        offensive_words = identify_offensive_words(text, contextual_analysis['context_type'])\n        \n        # Generate alternative\n        suggested_alternative = generate_alternative_with_t5(text, contextual_analysis, offensive_words)\n        \n        return {\n            'text': text,\n            'is_toxic': True,  # Direct insults are always toxic\n            'toxicity_scores': toxicity_scores,\n            'adjusted_scores': toxicity_scores,  # No adjustment for direct insults\n            'contextual_analysis': contextual_analysis,\n            'offensive_words': offensive_words,\n            'suggested_alternative': suggested_alternative,\n            'detection_method': 'pattern_matching'  # Indicate we used pattern matching\n        }\n    \n    # If not a direct insult, proceed with model prediction\n    # Tokenize text\n    inputs = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    # Move inputs to device\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.sigmoid(logits).cpu().numpy()[0]\n    \n    # Determine toxicity labels\n    toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    results = {label: float(prob) for label, prob in zip(toxicity_labels, probs)}\n    \n    # Analyze context\n    contextual_analysis = analyze_context(text, results)\n    \n    # Get context type\n    context_type = contextual_analysis['context_type']\n    \n    # Identify offensive words with context awareness\n    offensive_words = identify_offensive_words(text, context_type)\n    \n    # EDIT: Improved toxicity determination logic\n    adjusted_scores = contextual_analysis['adjusted_scores']\n    \n    # EDIT: Different thresholds based on context\n    threshold = 0.5\n    if context_type['educational']:\n        threshold = 0.7  # Higher threshold for educational content\n    elif context_type['condemning']:\n        threshold = 0.8  # Even higher threshold for condemning content\n    elif context_type['quoting']:\n        threshold = 0.7  # Higher threshold for quoting content\n    \n    # EDIT: More nuanced toxicity determination\n    is_toxic = False\n    \n    # Direct insults are always toxic\n    if context_type['direct_insult']:\n        is_toxic = True\n    # Check if any adjusted score exceeds the threshold\n    elif any(prob >= threshold for prob in adjusted_scores.values()):\n        is_toxic = True\n    # Check for offensive words, but respect context\n    elif offensive_words:\n        # In educational/condemning/quoting contexts, only consider highly offensive words\n        if (context_type['educational'] or context_type['condemning'] or context_type['quoting']):\n            is_toxic = any(score >= 0.9 for score in offensive_words.values())\n        else:\n            is_toxic = True\n    \n    # EDIT: Special case handling for common condemning phrases\n    special_phrases = [\n        \"rape is bad\", \"rape is wrong\", \"rape is terrible\", \n        \"sexual assault is bad\", \"sexual assault is wrong\",\n        \"racism is bad\", \"racism is wrong\", \"sexism is bad\", \"sexism is wrong\",\n        \"hate speech is bad\", \"hate speech is wrong\"\n    ]\n    \n    if any(phrase in text.lower() for phrase in special_phrases):\n        is_toxic = False\n    \n    # Generate alternative if toxic\n    suggested_alternative = None\n    if is_toxic:\n        suggested_alternative = generate_alternative_with_t5(text, contextual_analysis, offensive_words)\n    \n    return {\n        'text': text,\n        'is_toxic': is_toxic,\n        'toxicity_scores': results,\n        'adjusted_scores': adjusted_scores,\n        'contextual_analysis': contextual_analysis,\n        'offensive_words': offensive_words,\n        'suggested_alternative': suggested_alternative,\n        'detection_method': 'model_prediction'  # Indicate we used model prediction\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:25:26.414436Z","iopub.execute_input":"2025-05-06T14:25:26.414812Z","iopub.status.idle":"2025-05-06T14:25:26.434660Z","shell.execute_reply.started":"2025-05-06T14:25:26.414791Z","shell.execute_reply":"2025-05-06T14:25:26.433819Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"#Add a new function to detect direct insults\ndef is_direct_insult(text):\n    \"\"\"\n    Detect if text contains a direct insult using pattern matching\n    \"\"\"\n    direct_insult_patterns = [\n        r'you(\\'re| are) (a |an |such a )?(fucking |damn |stupid |idiotic |dumb |)?(idiot|stupid|dumb|moron|asshole|bitch|cunt|bastard|retard)',\n        r'(fuck|screw|damn) you',\n        r'i hate you',\n        r'(go|get) (fuck|screw) yourself',\n        r'(go|get) lost',\n        r'(go|get) (the fuck)? out( of here)?',\n        r'kill yourself',\n        r'die',\n        r'nobody (likes|cares about) you',\n        r'you(\\'re| are) (worthless|useless|pathetic)',\n        r'you (suck|stink)',\n        r'you(\\'re| are) (a |an )?(fucking |damn |stupid |)?(joke|loser|failure)',\n        r'shut (the fuck )?up',\n        r'shut your (mouth|face)',\n        r'(fuck|screw|damn) (off|you)',\n        r'(you|your) (stupid|dumb|idiotic|moronic) (fuck|ass|bitch)',\n        r'what (the fuck|the hell) (is|are) (you|this)'\n    ]\n    \n    return any(re.search(pattern, text.lower()) for pattern in direct_insult_patterns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:25:53.622849Z","iopub.execute_input":"2025-05-06T14:25:53.623359Z","iopub.status.idle":"2025-05-06T14:25:53.627896Z","shell.execute_reply.started":"2025-05-06T14:25:53.623341Z","shell.execute_reply":"2025-05-06T14:25:53.627215Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def simple_word_replacement(text, offensive_words):\n    \"\"\"\n    Enhanced simple word replacement with better alternatives and context preservation\n    \"\"\"\n    # EDIT: Expanded map of offensive words to non-offensive alternatives\n    offensive_to_neutral = {\n        'fuck': 'darn', 'fucking': 'really', 'fucked': 'messed up', 'fucker': 'person',\n        'shit': 'stuff', 'shitty': 'poor', 'bullshit': 'nonsense',\n        'ass': 'behind', 'asshole': 'jerk', 'jackass': 'fool',\n        'bitch': 'difficult person', 'bitches': 'people', 'bitching': 'complaining',\n        'damn': 'darn', 'goddamn': 'darn',\n        'nigga': 'person', 'nigger': 'person', 'negro': 'person',\n        'whore': 'person', 'slut': 'person', 'hoe': 'person',\n        'cunt': 'person', 'pussy': 'coward', 'dick': 'jerk', 'cock': 'rooster',\n        'retard': 'person', 'retarded': 'inappropriate',\n        'faggot': 'person', 'fag': 'person', 'homo': 'person',\n        'idiot': 'foolish person', 'stupid': 'unwise', 'dumb': 'uninformed', 'moron': 'foolish person',\n        'kill': 'harm', 'die': 'leave', 'suicide': 'self-harm',\n        'hate': 'dislike', 'loser': 'unfortunate person',\n        # EDIT: Added more alternatives for contextual words\n        'rape': 'sexual assault', \n        'nazi': 'extremist',\n        'terrorist': 'extremist',\n        'genocide': 'mass killing',\n        'slavery': 'forced labor',\n        'holocaust': 'genocide'\n    }\n    \n    # EDIT: Improved tokenization to better preserve sentence structure\n    tokens = []\n    current_token = \"\"\n    for char in text:\n        if char.isalnum() or char == \"'\":\n            current_token += char\n        else:\n            if current_token:\n                tokens.append(current_token)\n                current_token = \"\"\n            if not char.isspace():\n                tokens.append(char)\n    if current_token:\n        tokens.append(current_token)\n    \n    # EDIT: Improved replacement logic with phrase detection\n    i = 0\n    while i < len(tokens):\n        # Check for multi-word phrases (up to 3 words)\n        for phrase_len in range(3, 0, -1):\n            if i + phrase_len <= len(tokens):\n                # Create potential phrase\n                potential_phrase = \" \".join([t.lower() for t in tokens[i:i+phrase_len] \n                                           if t not in string.punctuation])\n                \n                if potential_phrase in offensive_to_neutral:\n                    # Replace with alternative\n                    replacement = offensive_to_neutral[potential_phrase]\n                    \n                    # Preserve capitalization of first token\n                    if tokens[i][0].isupper() if tokens[i] and tokens[i][0].isalpha() else False:\n                        replacement = replacement.capitalize()\n                    \n                    # Replace the first token with the alternative\n                    tokens[i] = replacement\n                    \n                    # Remove the other tokens that were part of the phrase\n                    for _ in range(phrase_len - 1):\n                        if i + 1 < len(tokens):\n                            tokens.pop(i + 1)\n                    \n                    break\n        \n        # Check single tokens\n        token_lower = tokens[i].lower()\n        if token_lower in offensive_to_neutral:\n            # Preserve capitalization\n            replacement = offensive_to_neutral[token_lower]\n            if tokens[i].istitle():\n                replacement = replacement.capitalize()\n            elif tokens[i].isupper():\n                replacement = replacement.upper()\n            tokens[i] = replacement\n        \n        i += 1\n    \n    # Reconstruct the text with proper spacing\n    result = \"\"\n    for i, token in enumerate(tokens):\n        if i > 0 and token in string.punctuation:\n            result += token\n        elif i > 0:\n            result += \" \" + token\n        else:\n            result += token\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:10:51.647780Z","iopub.execute_input":"2025-05-06T14:10:51.648071Z","iopub.status.idle":"2025-05-06T14:10:51.660853Z","shell.execute_reply.started":"2025-05-06T14:10:51.648052Z","shell.execute_reply":"2025-05-06T14:10:51.659911Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Function to predict toxicity with contextual understanding\ndef predict_toxicity(text, model, tokenizer, device):\n    \"\"\"\n    Enhanced toxicity prediction with better threshold handling and context awareness\n    \"\"\"\n    # Tokenize text\n    inputs = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    # Move inputs to device\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.sigmoid(logits).cpu().numpy()[0]\n    \n    # Determine toxicity labels\n    toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    results = {label: float(prob) for label, prob in zip(toxicity_labels, probs)}\n    \n    # Analyze context\n    contextual_analysis = analyze_context(text, results)\n    \n    # EDIT: First determine context before identifying offensive words\n    context_type = contextual_analysis['context_type']\n    \n    # Identify offensive words with context awareness\n    offensive_words = identify_offensive_words(text, context_type)\n    \n    # EDIT: Improved toxicity determination logic\n    adjusted_scores = contextual_analysis['adjusted_scores']\n    \n    # EDIT: Different thresholds based on context\n    threshold = 0.5\n    if context_type['educational']:\n        threshold = 0.7  # Higher threshold for educational content\n    elif context_type['condemning']:\n        threshold = 0.8  # Even higher threshold for condemning content\n    elif context_type['quoting']:\n        threshold = 0.7  # Higher threshold for quoting content\n    \n    # EDIT: More nuanced toxicity determination\n    is_toxic = False\n    \n    # Check if any adjusted score exceeds the threshold\n    if any(prob >= threshold for prob in adjusted_scores.values()):\n        is_toxic = True\n    \n    # Check for offensive words, but respect context\n    if offensive_words:\n        # In educational/condemning/quoting contexts, only consider highly offensive words\n        if (context_type['educational'] or context_type['condemning'] or context_type['quoting']):\n            is_toxic = any(score >= 0.9 for score in offensive_words.values())\n        else:\n            is_toxic = True\n    \n    # EDIT: Special case handling for common condemning phrases\n    special_phrases = [\n        \"rape is bad\", \"rape is wrong\", \"rape is terrible\", \n        \"sexual assault is bad\", \"sexual assault is wrong\",\n        \"racism is bad\", \"racism is wrong\", \"sexism is bad\", \"sexism is wrong\",\n        \"hate speech is bad\", \"hate speech is wrong\"\n    ]\n    \n    if any(phrase in text.lower() for phrase in special_phrases):\n        is_toxic = False\n    \n    # Generate alternative if toxic\n    suggested_alternative = None\n    if is_toxic:\n        suggested_alternative = generate_alternative_with_t5(text, contextual_analysis, offensive_words)\n    \n    return {\n        'text': text,\n        'is_toxic': is_toxic,\n        'toxicity_scores': results,\n        'adjusted_scores': adjusted_scores,\n        'contextual_analysis': contextual_analysis,\n        'offensive_words': offensive_words,\n        'suggested_alternative': suggested_alternative\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:11:11.770765Z","iopub.execute_input":"2025-05-06T14:11:11.771116Z","iopub.status.idle":"2025-05-06T14:11:11.783352Z","shell.execute_reply.started":"2025-05-06T14:11:11.771092Z","shell.execute_reply":"2025-05-06T14:11:11.782468Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def interactive_prediction(model, tokenizer, device):\n    \"\"\"\n    Enhanced interactive prediction with better output formatting\n    \"\"\"\n    print(\"\\n=== Offensive Content Detection Interactive Tool ===\")\n    print(\"Enter text to analyze (type 'exit' to quit):\")\n    \n    while True:\n        text = input(\"\\nEnter text: \")\n        if text.lower() == 'exit':\n            break\n        \n        # EDIT: First check if it's a direct insult\n        direct_insult = is_direct_insult(text)\n        if direct_insult:\n            print(\"\\nDirect insult detected! Skipping model prediction.\")\n        \n        # Get full prediction\n        result = predict_toxicity(text, model, tokenizer, device)\n        \n        print(\"\\n--- Analysis Results ---\")\n        print(f\"Text: {result['text']}\")\n        print(f\"Is offensive: {'Yes' if result['is_toxic'] else 'No'}\")\n        print(f\"Detection method: {result['detection_method']}\")\n        \n        print(\"\\nToxicity scores:\")\n        for label, score in result['toxicity_scores'].items():\n            print(f\"  {label}: {score:.4f}\")\n        \n        print(\"\\nAdjusted scores (after contextual analysis):\")\n        for label, score in result['adjusted_scores'].items():\n            print(f\"  {label}: {score:.4f}\")\n        \n        print(\"\\nContextual analysis:\")\n        print(f\"  Context adjustment: {result['contextual_analysis']['context_adjustment']}\")\n        if result['contextual_analysis']['reasons']:\n            print(\"  Reasons:\")\n            for reason in result['contextual_analysis']['reasons']:\n                print(f\"    - {reason}\")\n        \n        # EDIT: Improved offensive words display\n        if result['offensive_words']:\n            print(\"\\nPotentially sensitive words detected:\")\n            for word, severity in result['offensive_words'].items():\n                severity_level = \"High\" if severity >= 0.8 else \"Medium\" if severity >= 0.5 else \"Low\"\n                print(f\"  {word} (severity: {severity:.2f} - {severity_level})\")\n            \n            # Explain why these words might not make the content offensive\n            if not result['is_toxic'] and result['offensive_words']:\n                print(\"\\n  Note: These words were detected but determined to be non-offensive in this context.\")\n        \n        # EDIT: Improved suggestion display\n        if result['is_toxic']:\n            print(\"\\nSuggested non-offensive alternative:\")\n            if result['suggested_alternative']:\n                print(f\"  \\\"{result['suggested_alternative']}\\\"\")\n            else:\n                # Better explanation when no suggestion is available\n                if result['contextual_analysis']['context_type']['educational']:\n                    print(\"  No suggestion provided as this appears to be educational content.\")\n                elif result['contextual_analysis']['context_type']['condemning']:\n                    print(\"  No suggestion provided as this appears to be condemning harmful behavior.\")\n                elif result['contextual_analysis']['context_type']['quoting']:\n                    print(\"  No suggestion provided as this appears to be quoting or reporting content.\")\n                else:\n                    print(\"  No viable suggestion available for this content.\")\n        \n        print(\"\\n\" + \"-\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:26:22.039120Z","iopub.execute_input":"2025-05-06T14:26:22.039402Z","iopub.status.idle":"2025-05-06T14:26:22.048735Z","shell.execute_reply.started":"2025-05-06T14:26:22.039384Z","shell.execute_reply":"2025-05-06T14:26:22.047968Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Run the interactive prediction tool\nprint(\"\\nLoading the model for interactive prediction...\")\ninteractive_prediction(model, tokenizer, device)\n\n# Final model evaluation on test set\nprint(\"\\nFinal model evaluation on test set:\")\npredictions, true_labels = evaluate(model, test_dataloader, device)\n\n# Calculate overall metrics\naccuracy = accuracy_score(true_labels.flatten(), predictions.flatten())\nprecision, recall, f1, _ = precision_recall_fscore_support(\n    true_labels.flatten(), \n    predictions.flatten(), \n    average='binary'\n)\n\nprint(f\"Overall: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n\n# Calculate per-category metrics\ncategories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor i, category in enumerate(categories):\n    cat_precision, cat_recall, cat_f1, _ = precision_recall_fscore_support(\n        true_labels[:, i], \n        predictions[:, i], \n        average='binary'\n    )\n    cat_accuracy = accuracy_score(true_labels[:, i], predictions[:, i])\n    print(f\"{category}: Accuracy={cat_accuracy:.4f}, Precision={cat_precision:.4f}, Recall={cat_recall:.4f}, F1={cat_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:26:35.481506Z","iopub.execute_input":"2025-05-06T14:26:35.482200Z","iopub.status.idle":"2025-05-06T14:27:10.958661Z","shell.execute_reply.started":"2025-05-06T14:26:35.482178Z","shell.execute_reply":"2025-05-06T14:27:10.957523Z"}},"outputs":[{"name":"stdout","text":"\nLoading the model for interactive prediction...\n\n=== Offensive Content Detection Interactive Tool ===\nEnter text to analyze (type 'exit' to quit):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter text:  fuck you bitch ass nigga\n"},{"name":"stdout","text":"\nDirect insult detected! Skipping model prediction.\n\n--- Analysis Results ---\nText: fuck you bitch ass nigga\nIs offensive: Yes\nDetection method: pattern_matching\n\nToxicity scores:\n  toxic: 0.9000\n  severe_toxic: 0.7000\n  obscene: 0.8000\n  threat: 0.5000\n  insult: 0.9000\n  identity_hate: 0.5000\n\nAdjusted scores (after contextual analysis):\n  toxic: 0.9000\n  severe_toxic: 0.7000\n  obscene: 0.8000\n  threat: 0.5000\n  insult: 0.9000\n  identity_hate: 0.5000\n\nContextual analysis:\n  Context adjustment: 0\n  Reasons:\n    - Direct insult or attack detected\n\nPotentially sensitive words detected:\n  fuck (severity: 0.90 - High)\n  bitch (severity: 0.90 - High)\n  ass (severity: 0.70 - Medium)\n  nigga (severity: 1.00 - High)\n\nSuggested non-offensive alternative:\n  \"darn you difficult person behind person\"\n\n--------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3329258578.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the interactive prediction tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLoading the model for interactive prediction...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minteractive_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Final model evaluation on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/736152784.py\u001b[0m in \u001b[0;36minteractive_prediction\u001b[0;34m(model, tokenizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"# Example of handling difficult cases (Advanced contextual analysis)\nprint(\"\\nAdvanced contextual analysis examples:\")\n\ndifficult_examples = [\n    \"Rape is terrible and should never be tolerated in society.\",\n    \"The documentary discussed how hate speech affects minority communities.\",\n    \"According to the article, racial slurs were common in that historical period.\",\n    \"Research on offensive language shows interesting linguistic patterns.\",\n    \"He quoted the offensive message that said 'fuck you' to demonstrate harassment.\",\n    \"I hate you and wish you would die.\",\n    \"You're such a fucking idiot.\",\n    \"People from [demographic] are all [stereotype].\",\n    \"Go kill yourself.\",\n    \"What the fuck is this nigga talking about?\"\n]\n\nfor example in difficult_examples:\n    result = predict_toxicity(example, model, tokenizer, device)\n    \n    print(f\"\\nText: {example}\")\n    print(f\"Is offensive: {'Yes' if result['is_toxic'] else 'No'}\")\n    \n    if result['is_toxic'] and result['suggested_alternative']:\n        print(f\"Suggested alternative: {result['suggested_alternative']}\")\n    elif result['is_toxic']:\n        print(\"No viable suggestion available for this content.\")\n    \n    print(f\"Context analysis: {', '.join(result['contextual_analysis']['reasons']) if result['contextual_analysis']['reasons'] else 'No special context detected'}\")\n    print(\"-\"*50)\n\nprint(\"\\nThank you for using the Offensive Content Detection model!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:27:15.558354Z","iopub.execute_input":"2025-05-06T14:27:15.558736Z","iopub.status.idle":"2025-05-06T14:27:17.870150Z","shell.execute_reply.started":"2025-05-06T14:27:15.558709Z","shell.execute_reply":"2025-05-06T14:27:17.869588Z"}},"outputs":[{"name":"stdout","text":"\nAdvanced contextual analysis examples:\n\nText: Rape is terrible and should never be tolerated in society.\nIs offensive: No\nContext analysis: Condemning harmful behavior, Special case: condemning phrase 'rape is terrible', Multiple contextual indicators strengthen non-offensive determination\n--------------------------------------------------\n\nText: The documentary discussed how hate speech affects minority communities.\nIs offensive: No\nContext analysis: Educational context detected\n--------------------------------------------------\n\nText: According to the article, racial slurs were common in that historical period.\nIs offensive: No\nContext analysis: Educational context detected\n--------------------------------------------------\n\nText: Research on offensive language shows interesting linguistic patterns.\nIs offensive: No\nContext analysis: Educational context detected\n--------------------------------------------------\n\nText: He quoted the offensive message that said 'fuck you' to demonstrate harassment.\nIs offensive: Yes\nSuggested alternative: He quoted the offensive message that said 'fuck you' to demonstrate harassment.\nContext analysis: Direct insult or attack detected\n--------------------------------------------------\n\nText: I hate you and wish you would die.\nIs offensive: Yes\nSuggested alternative: I dislike you and wish you would leave\nContext analysis: Direct insult or attack detected\n--------------------------------------------------\n\nText: You're such a fucking idiot.\nIs offensive: Yes\nSuggested alternative: You're such a really foolish person\nContext analysis: Direct insult or attack detected\n--------------------------------------------------\n\nText: People from [demographic] are all [stereotype].\nIs offensive: No\nContext analysis: No special context detected\n--------------------------------------------------\n\nText: Go kill yourself.\nIs offensive: Yes\nSuggested alternative: Go harm yourself.\nContext analysis: Direct insult or attack detected\n--------------------------------------------------\n\nText: What the fuck is this nigga talking about?\nIs offensive: Yes\nSuggested alternative: What the darn is this person talking about?\nContext analysis: Direct insult or attack detected\n--------------------------------------------------\n\nThank you for using the Offensive Content Detection model!\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}